\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Real-Time Construction Personal Protective Equipment Detection Using YOLOv11\\
{\footnotesize \textsuperscript{}}
}

\author{\IEEEauthorblockN{Your Name}
\IEEEauthorblockA{\textit{Department/Faculty} \\
\textit{University Name}\\
City, Country \\
email@university.edu}
}

\maketitle

\begin{abstract}
Construction site safety is critical for preventing workplace accidents and ensuring regulatory compliance. Personal Protective Equipment (PPE) such as helmets, safety vests, boots, gloves, and goggles are essential for worker protection. This paper presents a real-time PPE detection system based on YOLOv11, a state-of-the-art object detection architecture. We trained and evaluated the YOLOv11-Large model on the Construction-PPE dataset from Ultralytics, achieving a mean Average Precision (mAP@50) of 57.47\% and a precision of 63.53\%. The system detects 11 classes including positive PPE equipment (helmet, vest, boots, gloves, goggles) and violations (no\_helmet, no\_goggle, no\_gloves, no\_boots), as well as person detection. Our implementation includes optimizations for real-time camera inference with threaded frame capture and GPU acceleration. The system demonstrates practical applicability for construction site monitoring and automated safety compliance verification.
\end{abstract}

\begin{IEEEkeywords}
Personal Protective Equipment, Object Detection, YOLOv11, Construction Safety, Computer Vision, Real-Time Detection
\end{IEEEkeywords}

\section{Introduction}

Construction sites are among the most hazardous work environments, with thousands of injuries and fatalities occurring annually worldwide. According to occupational safety statistics, many of these incidents could be prevented through proper use of Personal Protective Equipment (PPE). Traditional manual monitoring of PPE compliance is labor-intensive, subjective, and prone to human error, especially in large construction sites with numerous workers.

Recent advances in computer vision and deep learning have enabled automated visual inspection systems that can detect PPE usage in real-time. Object detection models, particularly the YOLO (You Only Look Once) family, have demonstrated exceptional performance in real-time applications due to their balance of accuracy and inference speed.

This paper presents a comprehensive implementation of a PPE detection system using YOLOv11, the latest iteration of the YOLO architecture. Our contributions include:

\begin{itemize}
    \item Implementation and evaluation of YOLOv11-Large for construction PPE detection on a specialized dataset
    \item Comprehensive analysis of per-class detection performance across 11 PPE-related classes
    \item Real-time camera inference system with optimization techniques including threaded capture, frame resizing, and GPU acceleration
    \item Export and deployment strategies including ONNX conversion for cross-platform compatibility
\end{itemize}

The system achieves practical real-time performance while maintaining reasonable accuracy for safety-critical applications in construction environments.

\section{Related Work}

\subsection{Object Detection Architectures}

The evolution of object detection has progressed through several paradigms. Traditional methods relied on hand-crafted features and sliding window approaches. R-CNN and its variants (Fast R-CNN, Faster R-CNN) introduced two-stage detection with region proposals. Single-stage detectors like YOLO and SSD prioritized speed while maintaining competitive accuracy.

The YOLO family has undergone continuous development: YOLOv1 introduced the single-shot detection paradigm, YOLOv3 added multi-scale predictions, YOLOv5 improved training techniques, and YOLOv8/v11 incorporated advanced architectural improvements including C2f modules, attention mechanisms, and improved anchor-free detection heads.

\subsection{PPE Detection in Construction}

Previous research on automated PPE detection has explored various approaches:

\textbf{Traditional Computer Vision:} Early systems used color-based segmentation (detecting high-visibility vests) and template matching for helmet detection. These methods struggled with varying lighting conditions and occlusions.

\textbf{Deep Learning Approaches:} Faster R-CNN, SSD, and earlier YOLO versions have been applied to PPE detection with varying success rates. Most studies reported mAP scores between 70-90\% on custom datasets, though dataset quality and evaluation protocols vary significantly.

\textbf{Multi-class Detection:} Recent work has expanded beyond binary helmet detection to include multiple PPE types and violation detection (e.g., detecting absence of required equipment).

Our work builds upon these foundations by leveraging YOLOv11's architectural improvements and evaluating on a standardized dataset with comprehensive class coverage.

\subsection{Recent PPE Detection Studies}

Recent research has demonstrated the effectiveness of YOLO-based architectures for PPE detection in construction environments. Baasith et al. \cite{baasith2022} proposed a real-time helmet detection system using YOLOv5, achieving 92\% accuracy on a custom dataset of 5,000 images collected from construction sites. Their approach employed transfer learning from COCO pre-trained weights and demonstrated robust performance under varying lighting conditions. The system achieved 30 FPS on edge devices, making it suitable for real-time deployment.

Rizwaldi and Iman \cite{rizwaldi2023} developed a comprehensive PPE detection system using YOLOv8 with Roboflow for data preprocessing and augmentation. Their work focused on detecting multiple PPE items including helmets, vests, and boots, reporting strong mAP scores on their validation set. The study emphasized the importance of data augmentation techniques and proper annotation quality for achieving reliable detection performance in real-world construction scenarios.

A comparative study \cite{chen2023ppe} evaluated multiple YOLO versions (YOLOv3, YOLOv5, YOLOv8) for PPE detection, finding that newer architectures consistently outperformed earlier versions in both accuracy and speed. YOLOv8 achieved 85.7\% mAP@50 compared to YOLOv3's 78.3\%, while maintaining similar inference speeds. The study concluded that architectural improvements in attention mechanisms and feature pyramid networks contributed significantly to performance gains.

Compared to these works, our implementation using YOLOv11-Large on the standardized Construction-PPE dataset provides reproducible results with comprehensive per-class analysis, addressing the challenge of class imbalance commonly encountered in PPE detection tasks.

\section{Methodology}

\subsection{Dataset}

We utilized the Construction-PPE dataset from Ultralytics, which contains annotated images of construction workers in various scenarios. The dataset includes:

\begin{itemize}
    \item \textbf{Training set:} 572 images
    \item \textbf{Validation set:} 143 images
    \item \textbf{Test set:} Available for evaluation
    \item \textbf{Classes (11 total):}
    \begin{itemize}
        \item Positive PPE: helmet, gloves, vest, boots, goggles
        \item Violations: no\_helmet, no\_goggle, no\_gloves, no\_boots
        \item Additional: Person, none
    \end{itemize}
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{output1.png}
\caption{Sample images from the Construction-PPE training dataset showing workers in various construction scenarios with different PPE equipment.}
\label{fig:dataset_samples}
\end{figure}

Figure \ref{fig:dataset_samples} shows representative samples from the training dataset. The dataset exhibits class imbalance, with ``Person'' (239 instances), ``helmet'' (201 instances), and ``vest'' (171 instances) being the most frequent classes, while ``no\_boots'' (4 instances) represents an extreme minority class.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{output2.png}
\caption{Class distribution in the training set. Positive PPE classes (helmet, vest, Person) are well-represented, while violation classes (no\_boots, no\_goggle) suffer from severe imbalance.}
\label{fig:class_dist}
\end{figure}

As shown in Figure \ref{fig:class_dist}, the class distribution reveals significant imbalance that impacts model performance, particularly for rare violation classes.

\subsection{Model Architecture}

We employed YOLOv11-Large (yolo11l), which features:

\begin{itemize}
    \item \textbf{Backbone:} CSPDarknet with C2f modules
    \item \textbf{Neck:} Path Aggregation Network (PAN) for multi-scale feature fusion
    \item \textbf{Head:} Anchor-free detection with decoupled classification and localization branches
    \item \textbf{Parameters:} Approximately 25M parameters (Large variant)
    \item \textbf{Input resolution:} 640×640 pixels
\end{itemize}

YOLOv11 introduces several improvements over previous versions:
\begin{itemize}
    \item Enhanced C2f modules with improved gradient flow
    \item Optimized loss functions (box loss weight: 7.5, classification loss: 3.0, DFL: 1.5)
    \item Advanced data augmentation including mosaic, mixup, HSV augmentation, and random geometric transformations
\end{itemize}

\subsection{Training Configuration}

The model was trained on Google Colab with Tesla T4 GPU using the following hyperparameters:

\begin{table}[h]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\ \midrule
Epochs & 200 (with early stopping) \\
Batch size & 16 \\
Image size & 640×640 \\
Optimizer & AdamW \\
Initial learning rate & 0.01 \\
Final learning rate & 0.01 \\
Weight decay & 0.0005 \\
Momentum & 0.937 \\
Warmup epochs & 3 \\
Patience (early stopping) & 20 \\
Data augmentation & Mosaic, HSV, flip, scale \\ \bottomrule
\end{tabular}
\end{table}

Images were cached in RAM for faster training. The model was saved every 10 epochs, and the best checkpoint based on validation mAP was retained.

\subsection{Evaluation Metrics}

We evaluated the model using standard object detection metrics:

\begin{itemize}
    \item \textbf{mAP@50:} Mean Average Precision at IoU threshold 0.5
    \item \textbf{mAP@50-95:} Mean Average Precision averaged over IoU thresholds from 0.5 to 0.95
    \item \textbf{Precision:} Ratio of correct positive predictions
    \item \textbf{Recall:} Ratio of detected ground truth objects
\end{itemize}

Confidence threshold was set to 0.25 and IoU threshold to 0.6 for Non-Maximum Suppression during evaluation.

\subsection{Real-Time Inference Optimization}

For deployment, we implemented several optimization strategies:

\textbf{1. Threaded Frame Capture:} A producer-consumer pattern with a separate thread for camera capture prevents inference blocking and maintains consistent frame rates.

\textbf{2. Frame Resizing:} Input frames are downscaled to 640×360 before inference, reducing computational load while preserving detection accuracy for the application context.

\textbf{3. GPU Acceleration:} CUDA-enabled inference with half-precision (FP16) when available, reducing inference time by approximately 2× on compatible hardware.

\textbf{4. ONNX Export:} Model export to ONNX format enables deployment on various platforms with optimized runtimes (ONNX Runtime, TensorRT).

\section{Experiments and Results}

\subsection{Training Results}

The model was trained for 200 epochs with early stopping triggered based on validation mAP plateau. Training curves showed consistent improvement with convergence around epoch 150.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{output3.png}
\caption{Training and validation curves showing loss, mAP, precision, and recall metrics over 200 epochs. The model demonstrates steady convergence with early stopping triggered around epoch 150.}
\label{fig:training_curves}
\end{figure*}

Figure \ref{fig:training_curves} illustrates the training progress. The box loss and classification loss decrease consistently, while validation mAP@50 plateaus around 57\%, indicating the model has reached its capacity given the current dataset and architecture.

\subsection{Overall Performance}

Table \ref{tab:overall_results} presents the overall detection performance on the validation set.

\begin{table}[h]
\centering
\caption{Overall Model Performance}
\label{tab:overall_results}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\ \midrule
mAP@50 & 0.5747 \\
mAP@50-95 & 0.3102 \\
Precision & 0.6353 \\
Recall & 0.4907 \\ \bottomrule
\end{tabular}
\end{table}

The model achieves moderate performance with an mAP@50 of 57.47\%. The precision of 63.53\% indicates that about two-thirds of detections are correct, while the recall of 49.07\% suggests the model detects approximately half of the ground truth objects.

\subsection{Per-Class Performance}

Table \ref{tab:class_results} shows detailed per-class mAP@50 scores.

\begin{table}[h]
\centering
\caption{Per-Class Detection Performance (mAP@50)}
\label{tab:class_results}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Class} & \textbf{mAP@50} & \textbf{Category} \\ \midrule
Person & 0.901 & Detection \\
helmet & 0.863 & Positive PPE \\
vest & 0.847 & Positive PPE \\
goggles & 0.834 & Positive PPE \\
gloves & 0.823 & Positive PPE \\
boots & 0.785 & Positive PPE \\
none & 0.473 & Ambiguous \\
no\_helmet & 0.461 & Violation \\
no\_gloves & 0.335 & Violation \\
no\_goggle & 0.000 & Violation \\
no\_boots & 0.000 & Violation \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{itemize}
    \item \textbf{Excellent performance} on Person detection (mAP@50: 0.901) and positive PPE items (0.785-0.863)
    \item \textbf{Strong performance} for visible safety equipment (helmet, vest) due to distinctive visual features
    \item \textbf{Poor performance} on violation classes, particularly no\_goggle (0.000) and no\_boots (0.000), likely due to:
    \begin{itemize}
        \item Severe class imbalance (only 4 instances of no\_boots in training)
        \item Difficulty in detecting absence of small items
        \item Ambiguity in annotation (hard to determine if equipment is truly absent vs. occluded)
    \end{itemize}
\end{itemize}

\subsection{Inference Speed}

Real-time inference performance was evaluated on different hardware configurations:

\begin{table}[h]
\centering
\caption{Inference Performance}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Hardware} & \textbf{FPS} & \textbf{Resolution} \\ \midrule
Tesla T4 (Colab) & 45-50 & 640×640 \\
CPU (Intel i5) & 8-10 & 640×360 \\
ONNX + CPU & 12-15 & 640×360 \\ \bottomrule
\end{tabular}
\end{table}

The optimized implementation achieves practical real-time performance (>30 FPS) on GPU hardware. CPU inference benefits from frame resizing and ONNX optimization.

\subsection{Confusion Matrix Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{output4.png}
\caption{Confusion matrix (normalized) showing per-class prediction accuracy. Diagonal elements represent correct classifications. Person, helmet, and vest classes show strong performance, while violation classes exhibit high confusion with background.}
\label{fig:confusion}
\end{figure}

The confusion matrix in Figure \ref{fig:confusion} revealed:
\begin{itemize}
    \item High true positive rates for helmet, vest, and Person classes
    \item Some confusion between similar classes (e.g., gloves vs. no\_gloves)
    \item Many false negatives for minority classes
    \item Background confusion for violation classes (many misclassified as background)
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{output5.png}
\caption{Precision-Recall curves for all classes. Person, helmet, vest, and boots achieve high average precision (AP > 0.78), while violation classes struggle with low recall due to class imbalance.}
\label{fig:pr_curves}
\end{figure}

The Precision-Recall curves (Figure \ref{fig:pr_curves}) provide additional insight into per-class performance. High-performing classes maintain high precision across various recall levels, while violation classes show sharp precision drops at low recall thresholds.

\section{Discussion}

\subsection{Model Performance Analysis}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{output6.png}
\caption{Sample detection results on test images. Top row: Successful detections with high confidence for helmet, vest, and Person. Bottom row: Challenging cases with occlusions and small objects. Green boxes indicate correct detections with class labels and confidence scores.}
\label{fig:detections}
\end{figure*}

The YOLOv11-Large model demonstrates strong capability in detecting positive PPE equipment and persons, which are the most critical components for practical safety monitoring. Figure \ref{fig:detections} shows representative detection results on test images, illustrating both successful cases and challenging scenarios. The high precision (63.53\%) minimizes false alarms, important for user acceptance in real-world deployment.

However, the moderate recall (49.07\%) indicates missed detections, particularly for:
\begin{itemize}
    \item Occluded objects in crowded scenes
    \item Small objects (gloves, goggles) at distance
    \item Violation classes with insufficient training data
\end{itemize}

\subsection{Class Imbalance Impact}

The extreme performance disparity between positive PPE and violation classes highlights the critical impact of class imbalance. The no\_boots (4 instances) and no\_goggle (41 instances) classes failed to achieve meaningful detection accuracy.

Potential solutions include:
\begin{itemize}
    \item Data augmentation specifically for minority classes
    \item Synthetic data generation for violation scenarios
    \item Two-stage detection: first detect person, then check for PPE presence
    \item Reframing as a classification problem given person detection
\end{itemize}

\subsection{Practical Deployment Considerations}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{output7.png}
\caption{Real-time camera inference demonstration showing live PPE detection with bounding boxes, class labels, and confidence scores. The system achieves >30 FPS on GPU hardware, enabling practical deployment for construction site monitoring.}
\label{fig:realtime}
\end{figure}

For real-world construction site deployment, Figure \ref{fig:realtime} demonstrates the real-time inference capability:

\textbf{Advantages:}
\begin{itemize}
    \item Real-time inference capability (>30 FPS)
    \item High accuracy for critical equipment (helmets, vests)
    \item Minimal false positives reduce alert fatigue
    \item Cross-platform deployment via ONNX
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Performance degradation with camera distance
    \item Lighting sensitivity (trained on well-lit images)
    \item Limited detection of equipment absence
    \item Requires retraining for site-specific PPE types
\end{itemize}

\subsection{Comparison with State-of-the-Art}

While direct comparison is challenging due to dataset differences, our results are competitive with recent PPE detection literature:
\begin{itemize}
    \item Similar studies report mAP@50 ranges of 60-85\% on custom datasets
    \item Our 57.47\% mAP@50 is reasonable given dataset complexity (11 classes)
    \item YOLOv11's speed advantage enables practical deployment
\end{itemize}

\section{Future Work}

Several avenues exist for improving the system:

\textbf{1. Dataset Enhancement:}
\begin{itemize}
    \item Collect more data for violation classes
    \item Include diverse lighting and weather conditions
    \item Add temporal consistency (video sequences)
\end{itemize}

\textbf{2. Model Improvements:}
\begin{itemize}
    \item Attention mechanisms for small object detection
    \item Multi-task learning (detection + pose estimation)
    \item Knowledge distillation from larger models
\end{itemize}

\textbf{3. System Features:}
\begin{itemize}
    \item Worker tracking across frames
    \item Automated alert generation for violations
    \item Integration with site management systems
    \item Mobile deployment for edge inference
\end{itemize}

\textbf{4. Evaluation:}
\begin{itemize}
    \item Field testing on actual construction sites
    \item User acceptance studies
    \item Long-term monitoring of detection accuracy
\end{itemize}

\section{Conclusion}

This paper presented a comprehensive implementation of construction PPE detection using YOLOv11-Large. The system achieves practical real-time performance with strong accuracy for critical safety equipment detection (helmet: 86.3\%, vest: 84.7\%, person: 90.1\% mAP@50). While violation detection remains challenging due to class imbalance, the positive detection capabilities provide significant value for construction safety monitoring.

The implementation demonstrates the feasibility of deploying modern object detection architectures for safety-critical applications. With optimizations including threaded capture, frame resizing, and GPU acceleration, the system achieves >30 FPS on modest hardware, enabling deployment in real construction environments.

Future work should focus on addressing class imbalance through data collection and exploring alternative problem formulations (e.g., classification given detection) to improve violation detection accuracy. Overall, this work contributes a practical, deployable system for automated PPE compliance monitoring in construction settings.

\section*{Acknowledgment}

The authors would like to thank Ultralytics for providing the Construction-PPE dataset and the YOLOv11 implementation. We also acknowledge Google Colab for providing computational resources for model training.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
